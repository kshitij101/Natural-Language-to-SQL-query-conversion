{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # obtains tokens with a least 1 alphabet\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    return pattern.findall(text.lower())\n",
    "\n",
    "def mapping(tokens):\n",
    "    word_to_id = dict()\n",
    "    id_to_word = dict()\n",
    "\n",
    "    for i, token in enumerate(set(tokens)):\n",
    "        word_to_id[token] = i\n",
    "        id_to_word[i] = token\n",
    "\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "def generate_training_data(tokens, word_to_id, window_size):\n",
    "    N = len(tokens)\n",
    "    X, Y = [], []\n",
    "\n",
    "    for i in range(N):\n",
    "        nbr_inds = list(range(max(0, i - window_size), i)) + \\\n",
    "                   list(range(i + 1, min(N, i + window_size + 1)))\n",
    "        for j in nbr_inds:\n",
    "            X.append(word_to_id[tokens[i]])\n",
    "            Y.append(word_to_id[tokens[j]])\n",
    "            \n",
    "    X = np.array(X)\n",
    "    X = np.expand_dims(X, axis=0)\n",
    "    Y = np.array(Y)\n",
    "    Y = np.expand_dims(Y, axis=0)\n",
    "            \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"C:\\\\Kshitij\\\\DEV\\\\Datasets\\\\spider\\\\train_spider.json\"\n",
    "# data_Set = open(\"C:\\\\Kshitij\\\\DEV\\\\Datasets\\\\spider\\\\train_others.json\")\n",
    "queries =[]\n",
    "questions = []\n",
    "with open(filename, 'r') as f:\n",
    "    datastore = json.load(f)\n",
    "\n",
    "for i in range(len(datastore)):\n",
    "\tqueries.append(datastore[i]['query'])\n",
    "\tquestions.append(datastore[i]['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_samples = 50\n",
    "doc = ''\n",
    "for i in questions[:num_of_samples]:\n",
    "    doc += i + \" \"\n",
    "tokens = tokenize(doc)\n",
    "word_to_id, id_to_word = mapping(tokens)\n",
    "X, Y = generate_training_data(tokens, word_to_id, 3)\n",
    "vocab_size = len(id_to_word)\n",
    "m = Y.shape[1]\n",
    "# turn Y into one hot encoding\n",
    "Y_one_hot = np.zeros((vocab_size, m))\n",
    "Y_one_hot[Y.flatten(), np.arange(m)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_wrd_emb(vocab_size, emb_size):\n",
    "    \"\"\"\n",
    "    vocab_size: int. vocabulary size of your corpus or training data\n",
    "    emb_size: int. word embedding size. How many dimensions to represent each vocabulary\n",
    "    \"\"\"\n",
    "    WRD_EMB = np.random.randn(vocab_size, emb_size) * 0.01\n",
    "    \n",
    "    assert(WRD_EMB.shape == (vocab_size, emb_size))\n",
    "    return WRD_EMB\n",
    "\n",
    "def initialize_dense(input_size, output_size):\n",
    "    \"\"\"\n",
    "    input_size: int. size of the input to the dense layer\n",
    "    output_szie: int. size of the output out of the dense layer\n",
    "    \"\"\"\n",
    "    W = np.random.randn(output_size, input_size) * 0.01\n",
    "    \n",
    "    assert(W.shape == (output_size, input_size))\n",
    "    return W\n",
    "\n",
    "def initialize_parameters(vocab_size, emb_size):\n",
    "    WRD_EMB = initialize_wrd_emb(vocab_size, emb_size)\n",
    "    W = initialize_dense(emb_size, vocab_size)\n",
    "    \n",
    "    parameters = {}\n",
    "    parameters['WRD_EMB'] = WRD_EMB\n",
    "    parameters['W'] = W\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ind_to_word_vecs(inds, parameters):\n",
    "    \"\"\"\n",
    "    inds: numpy array. shape: (1, m)\n",
    "    parameters: dict. weights to be trained\n",
    "    \"\"\"\n",
    "    m = inds.shape[1]\n",
    "    WRD_EMB = parameters['WRD_EMB']\n",
    "    word_vec = WRD_EMB[inds.flatten(), :].T\n",
    "    \n",
    "    assert(word_vec.shape == (WRD_EMB.shape[1], m))\n",
    "    \n",
    "    return word_vec\n",
    "\n",
    "def linear_dense(word_vec, parameters):\n",
    "    \"\"\"\n",
    "    word_vec: numpy array. shape: (emb_size, m)\n",
    "    parameters: dict. weights to be trained\n",
    "    \"\"\"\n",
    "    m = word_vec.shape[1]\n",
    "    W = parameters['W']\n",
    "    Z = np.dot(W, word_vec)\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], m))\n",
    "    \n",
    "    return W, Z\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Z: output out of the dense layer. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    softmax_out = np.divide(np.exp(Z), np.sum(np.exp(Z), axis=0, keepdims=True) + 0.001)\n",
    "    \n",
    "    assert(softmax_out.shape == Z.shape)\n",
    "\n",
    "    return softmax_out\n",
    "\n",
    "def forward_propagation(inds, parameters):\n",
    "    word_vec = ind_to_word_vecs(inds, parameters)\n",
    "    W, Z = linear_dense(word_vec, parameters)\n",
    "    softmax_out = softmax(Z)\n",
    "    \n",
    "    caches = {}\n",
    "    caches['inds'] = inds\n",
    "    caches['word_vec'] = word_vec\n",
    "    caches['W'] = W\n",
    "    caches['Z'] = Z\n",
    "    \n",
    "    return softmax_out, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(softmax_out, Y):\n",
    "    \"\"\"\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    m = softmax_out.shape[1]\n",
    "    cost = -(1 / m) * np.sum(np.sum(Y * np.log(softmax_out + 0.001), axis=0, keepdims=True), axis=1)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(Y, softmax_out):\n",
    "    \"\"\"\n",
    "    Y: labels of training data. shape: (vocab_size, m)\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    dL_dZ = softmax_out - Y //predicted - actual\n",
    "    \n",
    "    //is\n",
    "    assert(dL_dZ.shape == softmax_out.shape)\n",
    "    return dL_dZ\n",
    "\n",
    "def dense_backward(dL_dZ, caches):\n",
    "    \"\"\"\n",
    "    dL_dZ: shape: (vocab_size, m)\n",
    "    caches: dict. results from each steps of forward propagation\n",
    "    \"\"\"\n",
    "    W = caches['W']\n",
    "    word_vec = caches['word_vec']\n",
    "    m = word_vec.shape[1]\n",
    "    \n",
    "    dL_dW = (1 / m) * np.dot(dL_dZ, word_vec.T)\n",
    "    dL_dword_vec = np.dot(W.T, dL_dZ)\n",
    "    \n",
    "    assert(W.shape == dL_dW.shape)\n",
    "    assert(word_vec.shape == dL_dword_vec.shape)\n",
    "    \n",
    "    return dL_dW, dL_dword_vec\n",
    "\n",
    "def backward_propagation(Y, softmax_out, caches):\n",
    "    dL_dZ = softmax_backward(Y, softmax_out)\n",
    "    dL_dW, dL_dword_vec = dense_backward(dL_dZ, caches)\n",
    "    \n",
    "    gradients = dict()\n",
    "    gradients['dL_dZ'] = dL_dZ\n",
    "    gradients['dL_dW'] = dL_dW\n",
    "    gradients['dL_dword_vec'] = dL_dword_vec\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def update_parameters(parameters, caches, gradients, learning_rate):\n",
    "    vocab_size, emb_size = parameters['WRD_EMB'].shape\n",
    "    inds = caches['inds']\n",
    "    dL_dword_vec = gradients['dL_dword_vec']\n",
    "    m = inds.shape[-1]\n",
    "    \n",
    "    parameters['WRD_EMB'][inds.flatten(), :] -= dL_dword_vec.T * learning_rate\n",
    "\n",
    "    parameters['W'] -= learning_rate * gradients['dL_dW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def skipgram_model_training(X, Y, vocab_size, emb_size, learning_rate, epochs, batch_size=256, parameters=None, print_cost=False, plot_cost=True):\n",
    "    costs = []\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    if parameters is None:\n",
    "        parameters = initialize_parameters(vocab_size, emb_size)\n",
    "    \n",
    "    begin_time = datetime.now()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_cost = 0\n",
    "        batch_inds = list(range(0, m, batch_size))\n",
    "        np.random.shuffle(batch_inds)\n",
    "        for i in batch_inds:\n",
    "            X_batch = X[:, i:i+batch_size]\n",
    "            Y_batch = Y[:, i:i+batch_size]\n",
    "\n",
    "            softmax_out, caches = forward_propagation(X_batch, parameters)\n",
    "            gradients = backward_propagation(Y_batch, softmax_out, caches)\n",
    "            update_parameters(parameters, caches, gradients, learning_rate)\n",
    "            cost = cross_entropy(softmax_out, Y_batch)\n",
    "            epoch_cost += np.squeeze(cost)\n",
    "            \n",
    "        costs.append(epoch_cost)\n",
    "        if print_cost and epoch % (epochs // 500) == 0:\n",
    "            print(\"Cost after epoch {}: {}\".format(epoch, epoch_cost))\n",
    "        if epoch % (epochs // 100) == 0:\n",
    "            learning_rate *= 0.98\n",
    "    end_time = datetime.now()\n",
    "    print('training time: {}'.format(end_time - begin_time))\n",
    "            \n",
    "    if plot_cost:\n",
    "        plt.plot(np.arange(epochs), costs)\n",
    "        plt.xlabel('# of epochs')\n",
    "        plt.ylabel('cost')\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 131.69414722966994\n",
      "Cost after epoch 10: 131.6232920440272\n",
      "Cost after epoch 20: 130.72387058438102\n",
      "Cost after epoch 30: 125.24886548169775\n",
      "Cost after epoch 40: 121.56748164805818\n",
      "Cost after epoch 50: 119.51177453622606\n",
      "Cost after epoch 60: 118.32737832136804\n",
      "Cost after epoch 70: 117.55273102621275\n",
      "Cost after epoch 80: 116.91511727158358\n",
      "Cost after epoch 90: 116.39095979664306\n",
      "Cost after epoch 100: 115.92397062347716\n",
      "Cost after epoch 110: 115.41691515368747\n",
      "Cost after epoch 120: 114.89899352645716\n",
      "Cost after epoch 130: 114.31744838778947\n",
      "Cost after epoch 140: 113.72925786837214\n",
      "Cost after epoch 150: 113.11742689832482\n",
      "Cost after epoch 160: 112.49879909093417\n",
      "Cost after epoch 170: 111.895708285623\n",
      "Cost after epoch 180: 111.34449715084364\n",
      "Cost after epoch 190: 110.74532677819764\n",
      "Cost after epoch 200: 110.24517017541146\n",
      "Cost after epoch 210: 109.79859054397777\n",
      "Cost after epoch 220: 109.38045439862529\n",
      "Cost after epoch 230: 108.99804128748849\n",
      "Cost after epoch 240: 108.62273489841007\n",
      "Cost after epoch 250: 108.28288509956435\n",
      "Cost after epoch 260: 107.93118121736543\n",
      "Cost after epoch 270: 107.60218692577028\n",
      "Cost after epoch 280: 107.32291127895827\n",
      "Cost after epoch 290: 106.98058990892419\n",
      "Cost after epoch 300: 106.74932147966095\n",
      "Cost after epoch 310: 106.5477555367777\n",
      "Cost after epoch 320: 106.2796840995024\n",
      "Cost after epoch 330: 106.13702241323303\n",
      "Cost after epoch 340: 105.86298918197402\n",
      "Cost after epoch 350: 105.79778548186748\n",
      "Cost after epoch 360: 105.6722788376673\n",
      "Cost after epoch 370: 105.42813513967442\n",
      "Cost after epoch 380: 105.33994532127282\n",
      "Cost after epoch 390: 105.10190369933743\n",
      "Cost after epoch 400: 105.10812854595248\n",
      "Cost after epoch 410: 104.98709528029539\n",
      "Cost after epoch 420: 104.88533937951534\n",
      "Cost after epoch 430: 104.79487269367837\n",
      "Cost after epoch 440: 104.68952120345794\n",
      "Cost after epoch 450: 104.66405941346139\n",
      "Cost after epoch 460: 104.53411654718171\n",
      "Cost after epoch 470: 104.50136867404741\n",
      "Cost after epoch 480: 104.43635109509646\n",
      "Cost after epoch 490: 104.40178970307345\n",
      "Cost after epoch 500: 104.37691136346102\n",
      "Cost after epoch 510: 104.21224544602862\n",
      "Cost after epoch 520: 104.24488781999864\n",
      "Cost after epoch 530: 104.19749261303836\n",
      "Cost after epoch 540: 104.10561453103361\n",
      "Cost after epoch 550: 104.08990666782343\n",
      "Cost after epoch 560: 104.0949360128466\n",
      "Cost after epoch 570: 104.10822466404625\n",
      "Cost after epoch 580: 104.15251525155631\n",
      "Cost after epoch 590: 104.12338091731354\n",
      "Cost after epoch 600: 104.01676313525921\n",
      "Cost after epoch 610: 104.01969068262429\n",
      "Cost after epoch 620: 103.94561457057893\n",
      "Cost after epoch 630: 103.9569190483566\n",
      "Cost after epoch 640: 103.94294096976714\n",
      "Cost after epoch 650: 103.86043075392236\n",
      "Cost after epoch 660: 103.89302276398844\n",
      "Cost after epoch 670: 103.80687555620979\n",
      "Cost after epoch 680: 103.81363948792736\n",
      "Cost after epoch 690: 103.75043366197679\n",
      "Cost after epoch 700: 103.7024968214043\n",
      "Cost after epoch 710: 103.72761019850843\n",
      "Cost after epoch 720: 103.77579693202685\n",
      "Cost after epoch 730: 103.65032087872657\n",
      "Cost after epoch 740: 103.74119994706227\n",
      "Cost after epoch 750: 103.76147341104885\n",
      "Cost after epoch 760: 103.73479629441319\n",
      "Cost after epoch 770: 103.64470092786385\n",
      "Cost after epoch 780: 103.78301052812614\n",
      "Cost after epoch 790: 103.66656651851666\n",
      "Cost after epoch 800: 103.81745286942363\n",
      "Cost after epoch 810: 103.8673190351284\n",
      "Cost after epoch 820: 103.65377072562534\n",
      "Cost after epoch 830: 103.82137003774847\n",
      "Cost after epoch 840: 103.78739441954878\n",
      "Cost after epoch 850: 103.79807074257812\n",
      "Cost after epoch 860: 103.76790997326772\n",
      "Cost after epoch 870: 103.65105206297002\n",
      "Cost after epoch 880: 103.63324704303616\n",
      "Cost after epoch 890: 103.7457964444494\n",
      "Cost after epoch 900: 103.77772789937373\n",
      "Cost after epoch 910: 103.76362308800466\n",
      "Cost after epoch 920: 103.74628584920055\n",
      "Cost after epoch 930: 103.77478379529875\n",
      "Cost after epoch 940: 103.76811027189618\n",
      "Cost after epoch 950: 103.7568858704493\n",
      "Cost after epoch 960: 103.74051282144995\n",
      "Cost after epoch 970: 103.82318028052302\n",
      "Cost after epoch 980: 103.86168435612626\n",
      "Cost after epoch 990: 103.94830100578255\n",
      "Cost after epoch 1000: 103.88267226242623\n",
      "Cost after epoch 1010: 103.85827401801419\n",
      "Cost after epoch 1020: 103.87085460705154\n",
      "Cost after epoch 1030: 103.788633511286\n",
      "Cost after epoch 1040: 103.76702820754707\n",
      "Cost after epoch 1050: 103.8628265548897\n",
      "Cost after epoch 1060: 103.8812977002289\n",
      "Cost after epoch 1070: 103.94253623560121\n",
      "Cost after epoch 1080: 103.71183316731333\n",
      "Cost after epoch 1090: 103.92228219842286\n",
      "Cost after epoch 1100: 104.06344157333642\n",
      "Cost after epoch 1110: 103.8124185990038\n",
      "Cost after epoch 1120: 103.83705661762117\n",
      "Cost after epoch 1130: 103.89564332233608\n",
      "Cost after epoch 1140: 103.92662886521987\n",
      "Cost after epoch 1150: 103.94687424761771\n",
      "Cost after epoch 1160: 103.90194312602888\n",
      "Cost after epoch 1170: 103.81547651705779\n",
      "Cost after epoch 1180: 103.76636807560021\n",
      "Cost after epoch 1190: 103.86738293344396\n",
      "Cost after epoch 1200: 103.81826073279636\n",
      "Cost after epoch 1210: 103.69374561076495\n",
      "Cost after epoch 1220: 103.69300618102712\n",
      "Cost after epoch 1230: 103.76787000495624\n",
      "Cost after epoch 1240: 103.68596709279396\n",
      "Cost after epoch 1250: 103.69299998255778\n",
      "Cost after epoch 1260: 103.68577030261625\n",
      "Cost after epoch 1270: 103.63087547636545\n",
      "Cost after epoch 1280: 103.68394177681051\n",
      "Cost after epoch 1290: 103.4468045452764\n",
      "Cost after epoch 1300: 103.67989064816395\n",
      "Cost after epoch 1310: 103.60855888593804\n",
      "Cost after epoch 1320: 103.61700417250333\n",
      "Cost after epoch 1330: 103.59674869478181\n",
      "Cost after epoch 1340: 103.75302193256505\n",
      "Cost after epoch 1350: 103.73595983152035\n",
      "Cost after epoch 1360: 103.66671677840405\n",
      "Cost after epoch 1370: 103.71013525745863\n",
      "Cost after epoch 1380: 103.7456866705552\n",
      "Cost after epoch 1390: 103.7011258064259\n",
      "Cost after epoch 1400: 103.81810421228995\n",
      "Cost after epoch 1410: 103.81841107235854\n",
      "Cost after epoch 1420: 103.8011722590839\n",
      "Cost after epoch 1430: 103.74894427555583\n",
      "Cost after epoch 1440: 103.63396908593819\n",
      "Cost after epoch 1450: 103.86320194080889\n",
      "Cost after epoch 1460: 103.7579468153388\n",
      "Cost after epoch 1470: 103.74897972278208\n",
      "Cost after epoch 1480: 103.77853538137113\n",
      "Cost after epoch 1490: 103.74799116848708\n",
      "Cost after epoch 1500: 103.76436351423094\n",
      "Cost after epoch 1510: 103.57313182945047\n",
      "Cost after epoch 1520: 103.58014714748234\n",
      "Cost after epoch 1530: 103.78050263663737\n",
      "Cost after epoch 1540: 103.70987229312564\n",
      "Cost after epoch 1550: 103.7397276307754\n",
      "Cost after epoch 1560: 103.69329677958034\n",
      "Cost after epoch 1570: 103.73339089788092\n",
      "Cost after epoch 1580: 103.5976614626015\n",
      "Cost after epoch 1590: 103.78029880778656\n",
      "Cost after epoch 1600: 103.68994510194851\n",
      "Cost after epoch 1610: 103.62353868604609\n",
      "Cost after epoch 1620: 103.68730460010734\n",
      "Cost after epoch 1630: 103.80623364631107\n",
      "Cost after epoch 1640: 103.75904695048904\n",
      "Cost after epoch 1650: 103.94135791754097\n",
      "Cost after epoch 1660: 103.84363873371711\n",
      "Cost after epoch 1670: 103.86556570450918\n",
      "Cost after epoch 1680: 103.82374596074607\n",
      "Cost after epoch 1690: 103.88525869849423\n",
      "Cost after epoch 1700: 103.77206810113898\n",
      "Cost after epoch 1710: 103.82604392336123\n",
      "Cost after epoch 1720: 104.00405922705268\n",
      "Cost after epoch 1730: 103.97269595015749\n",
      "Cost after epoch 1740: 103.94538525610561\n",
      "Cost after epoch 1750: 103.91602596749448\n",
      "Cost after epoch 1760: 103.78940038999244\n",
      "Cost after epoch 1770: 103.96226487388222\n",
      "Cost after epoch 1780: 103.91249197841942\n",
      "Cost after epoch 1790: 103.94897138916586\n",
      "Cost after epoch 1800: 103.81578711152542\n",
      "Cost after epoch 1810: 103.91542258915324\n",
      "Cost after epoch 1820: 103.89698744611647\n",
      "Cost after epoch 1830: 103.73231959170167\n",
      "Cost after epoch 1840: 103.58869786131078\n",
      "Cost after epoch 1850: 103.79746723173616\n",
      "Cost after epoch 1860: 103.565750557976\n",
      "Cost after epoch 1870: 103.71223836075929\n",
      "Cost after epoch 1880: 103.69692818868157\n",
      "Cost after epoch 1890: 103.70246444577344\n",
      "Cost after epoch 1900: 103.65996628884169\n",
      "Cost after epoch 1910: 103.64607998898715\n",
      "Cost after epoch 1920: 103.58691208140522\n",
      "Cost after epoch 1930: 103.61485967770706\n",
      "Cost after epoch 1940: 103.81964631778293\n",
      "Cost after epoch 1950: 103.71622439924664\n",
      "Cost after epoch 1960: 103.58734585103677\n",
      "Cost after epoch 1970: 103.73542436556562\n",
      "Cost after epoch 1980: 103.6070564535466\n",
      "Cost after epoch 1990: 103.70265786480944\n",
      "Cost after epoch 2000: 103.65171129977824\n",
      "Cost after epoch 2010: 103.73396988164554\n",
      "Cost after epoch 2020: 103.72973391581209\n",
      "Cost after epoch 2030: 103.64174335427319\n",
      "Cost after epoch 2040: 103.71816143462115\n",
      "Cost after epoch 2050: 103.52748827414732\n",
      "Cost after epoch 2060: 103.67085367332847\n",
      "Cost after epoch 2070: 103.62102894040672\n",
      "Cost after epoch 2080: 103.62859192713421\n",
      "Cost after epoch 2090: 103.76389363820388\n",
      "Cost after epoch 2100: 103.65103228239758\n",
      "Cost after epoch 2110: 103.69468553871245\n",
      "Cost after epoch 2120: 103.71661562291763\n",
      "Cost after epoch 2130: 103.68814509174953\n",
      "Cost after epoch 2140: 103.77122868723383\n",
      "Cost after epoch 2150: 103.60315225146424\n",
      "Cost after epoch 2160: 103.62359230147796\n",
      "Cost after epoch 2170: 103.7597834322021\n",
      "Cost after epoch 2180: 103.77598767909949\n",
      "Cost after epoch 2190: 103.63181294619477\n",
      "Cost after epoch 2200: 103.60173132071121\n",
      "Cost after epoch 2210: 103.51691329255333\n",
      "Cost after epoch 2220: 103.59246796272467\n",
      "Cost after epoch 2230: 103.66171329496375\n",
      "Cost after epoch 2240: 103.81724571720656\n",
      "Cost after epoch 2250: 103.7391051221204\n",
      "Cost after epoch 2260: 103.65089225820655\n",
      "Cost after epoch 2270: 103.74050686755007\n",
      "Cost after epoch 2280: 103.60634335033224\n",
      "Cost after epoch 2290: 103.70708867884713\n",
      "Cost after epoch 2300: 103.69363637440122\n",
      "Cost after epoch 2310: 103.77478950283313\n",
      "Cost after epoch 2320: 103.71601709022734\n",
      "Cost after epoch 2330: 103.83486822389969\n",
      "Cost after epoch 2340: 103.81223552840693\n",
      "Cost after epoch 2350: 103.80670869977061\n",
      "Cost after epoch 2360: 103.72087844365689\n",
      "Cost after epoch 2370: 103.60873903443819\n",
      "Cost after epoch 2380: 103.64676971684642\n",
      "Cost after epoch 2390: 103.74448128842057\n",
      "Cost after epoch 2400: 103.83964266176754\n",
      "Cost after epoch 2410: 103.7810078497514\n",
      "Cost after epoch 2420: 103.8328005001019\n",
      "Cost after epoch 2430: 103.89269229494941\n",
      "Cost after epoch 2440: 103.84098007697827\n",
      "Cost after epoch 2450: 103.85624698163919\n",
      "Cost after epoch 2460: 103.83762772040458\n",
      "Cost after epoch 2470: 103.81682337733507\n",
      "Cost after epoch 2480: 103.87223714103436\n",
      "Cost after epoch 2490: 103.82472895864983\n",
      "Cost after epoch 2500: 103.7918470233054\n",
      "Cost after epoch 2510: 103.82016283862771\n",
      "Cost after epoch 2520: 103.76866929714411\n",
      "Cost after epoch 2530: 103.76396854065277\n",
      "Cost after epoch 2540: 103.76524126478124\n",
      "Cost after epoch 2550: 103.71022479777851\n",
      "Cost after epoch 2560: 103.75440728268157\n",
      "Cost after epoch 2570: 103.63701048700374\n",
      "Cost after epoch 2580: 103.65204271749273\n",
      "Cost after epoch 2590: 103.66309226325787\n",
      "Cost after epoch 2600: 103.69397641808911\n",
      "Cost after epoch 2610: 103.64503958371479\n",
      "Cost after epoch 2620: 103.64728837450298\n",
      "Cost after epoch 2630: 103.51561218730066\n",
      "Cost after epoch 2640: 103.63655740304951\n",
      "Cost after epoch 2650: 103.42547518368903\n",
      "Cost after epoch 2660: 103.5634073964281\n",
      "Cost after epoch 2670: 103.59094349240645\n",
      "Cost after epoch 2680: 103.40180310911941\n",
      "Cost after epoch 2690: 103.57080393037258\n",
      "Cost after epoch 2700: 103.40372174494452\n",
      "Cost after epoch 2710: 103.39202697305085\n",
      "Cost after epoch 2720: 103.33823119410374\n",
      "Cost after epoch 2730: 103.44096066026803\n",
      "Cost after epoch 2740: 103.47367186589638\n",
      "Cost after epoch 2750: 103.28971875858142\n",
      "Cost after epoch 2760: 103.32953044711194\n",
      "Cost after epoch 2770: 103.20929436408899\n",
      "Cost after epoch 2780: 103.42545376502544\n",
      "Cost after epoch 2790: 103.19639617379833\n",
      "Cost after epoch 2800: 103.27232861901443\n",
      "Cost after epoch 2810: 103.2049542247859\n",
      "Cost after epoch 2820: 103.34270254364714\n",
      "Cost after epoch 2830: 103.34527777630174\n",
      "Cost after epoch 2840: 103.25426605203646\n",
      "Cost after epoch 2850: 103.33021302385029\n",
      "Cost after epoch 2860: 103.32835161542502\n",
      "Cost after epoch 2870: 103.45064097945351\n",
      "Cost after epoch 2880: 103.23831014092488\n",
      "Cost after epoch 2890: 103.30075689900073\n",
      "Cost after epoch 2900: 103.3496754591348\n",
      "Cost after epoch 2910: 103.23082305483048\n",
      "Cost after epoch 2920: 103.32397290440952\n",
      "Cost after epoch 2930: 103.25494052133074\n",
      "Cost after epoch 2940: 103.27833434028605\n",
      "Cost after epoch 2950: 103.21988051734928\n",
      "Cost after epoch 2960: 103.23528040191438\n",
      "Cost after epoch 2970: 103.077879604876\n",
      "Cost after epoch 2980: 103.22039155568928\n",
      "Cost after epoch 2990: 103.20259494657648\n",
      "Cost after epoch 3000: 103.33274904098218\n",
      "Cost after epoch 3010: 103.13870899788506\n",
      "Cost after epoch 3020: 103.28437237806963\n",
      "Cost after epoch 3030: 103.12007585249863\n",
      "Cost after epoch 3040: 103.1767877594371\n",
      "Cost after epoch 3050: 103.09524245098181\n",
      "Cost after epoch 3060: 103.1113382802001\n",
      "Cost after epoch 3070: 103.23915935788922\n",
      "Cost after epoch 3080: 103.03989207135534\n",
      "Cost after epoch 3090: 103.06624190993665\n",
      "Cost after epoch 3100: 103.01968499364982\n",
      "Cost after epoch 3110: 103.1228254428918\n",
      "Cost after epoch 3120: 103.03135095874725\n",
      "Cost after epoch 3130: 103.14060385397404\n",
      "Cost after epoch 3140: 103.14301700412663\n",
      "Cost after epoch 3150: 103.10890126608845\n",
      "Cost after epoch 3160: 103.09344669871717\n",
      "Cost after epoch 3170: 103.20410033418732\n",
      "Cost after epoch 3180: 103.19055294311922\n",
      "Cost after epoch 3190: 103.04019993175886\n",
      "Cost after epoch 3200: 103.18899891472539\n",
      "Cost after epoch 3210: 103.25823254333898\n",
      "Cost after epoch 3220: 103.19338183406856\n",
      "Cost after epoch 3230: 103.23179164841065\n",
      "Cost after epoch 3240: 103.19942268767511\n",
      "Cost after epoch 3250: 103.33370729865086\n",
      "Cost after epoch 3260: 103.18956541813324\n",
      "Cost after epoch 3270: 103.20860082643532\n",
      "Cost after epoch 3280: 103.30366785222279\n",
      "Cost after epoch 3290: 103.22635743721877\n",
      "Cost after epoch 3300: 103.30216084929421\n",
      "Cost after epoch 3310: 103.38994181912223\n",
      "Cost after epoch 3320: 103.23958653079328\n",
      "Cost after epoch 3330: 103.24009471503551\n",
      "Cost after epoch 3340: 103.2635579552661\n",
      "Cost after epoch 3350: 103.31911078728115\n",
      "Cost after epoch 3360: 103.30417611532553\n",
      "Cost after epoch 3370: 103.31472636245559\n",
      "Cost after epoch 3380: 103.3790819700529\n",
      "Cost after epoch 3390: 103.37861529636633\n",
      "Cost after epoch 3400: 103.30339324337206\n",
      "Cost after epoch 3410: 103.36594660358476\n",
      "Cost after epoch 3420: 103.28588411152023\n",
      "Cost after epoch 3430: 103.38483050921326\n",
      "Cost after epoch 3440: 103.30018570171484\n",
      "Cost after epoch 3450: 103.36365387016798\n",
      "Cost after epoch 3460: 103.3975881020944\n",
      "Cost after epoch 3470: 103.33392295515662\n",
      "Cost after epoch 3480: 103.27593830104162\n",
      "Cost after epoch 3490: 103.32762806110438\n",
      "Cost after epoch 3500: 103.29693467202671\n",
      "Cost after epoch 3510: 103.32068208207131\n",
      "Cost after epoch 3520: 103.29661470792072\n",
      "Cost after epoch 3530: 103.22621323828113\n",
      "Cost after epoch 3540: 103.20984787380124\n",
      "Cost after epoch 3550: 103.18124720249845\n",
      "Cost after epoch 3560: 103.28104626867982\n",
      "Cost after epoch 3570: 103.24735415064389\n",
      "Cost after epoch 3580: 103.23370592608539\n",
      "Cost after epoch 3590: 103.23885252322079\n",
      "Cost after epoch 3600: 103.29771454092698\n",
      "Cost after epoch 3610: 103.25537004189071\n",
      "Cost after epoch 3620: 103.26374580561142\n",
      "Cost after epoch 3630: 103.2642910177089\n",
      "Cost after epoch 3640: 103.29702293854714\n",
      "Cost after epoch 3650: 103.32839379765821\n",
      "Cost after epoch 3660: 103.28828564389572\n",
      "Cost after epoch 3670: 103.2720208618769\n",
      "Cost after epoch 3680: 103.32125722528659\n",
      "Cost after epoch 3690: 103.26584977201374\n",
      "Cost after epoch 3700: 103.2418546169156\n",
      "Cost after epoch 3710: 103.16084448704837\n",
      "Cost after epoch 3720: 103.32589052028251\n",
      "Cost after epoch 3730: 103.27844167675194\n",
      "Cost after epoch 3740: 103.29728996417991\n",
      "Cost after epoch 3750: 103.3904238879768\n",
      "Cost after epoch 3760: 103.27307648279016\n",
      "Cost after epoch 3770: 103.35794275798807\n",
      "Cost after epoch 3780: 103.411763538074\n",
      "Cost after epoch 3790: 103.34198179184712\n",
      "Cost after epoch 3800: 103.31087586701318\n",
      "Cost after epoch 3810: 103.33623024993823\n",
      "Cost after epoch 3820: 103.31107558636627\n",
      "Cost after epoch 3830: 103.3603967041201\n",
      "Cost after epoch 3840: 103.33796609645154\n",
      "Cost after epoch 3850: 103.47673691447824\n",
      "Cost after epoch 3860: 103.45938994395236\n",
      "Cost after epoch 3870: 103.40978520859403\n",
      "Cost after epoch 3880: 103.45274239173686\n",
      "Cost after epoch 3890: 103.38177678746467\n",
      "Cost after epoch 3900: 103.4148057314557\n",
      "Cost after epoch 3910: 103.48426106040242\n",
      "Cost after epoch 3920: 103.37307299569474\n",
      "Cost after epoch 3930: 103.52406285334074\n",
      "Cost after epoch 3940: 103.41002912504308\n",
      "Cost after epoch 3950: 103.46004091419157\n",
      "Cost after epoch 3960: 103.45918183674007\n",
      "Cost after epoch 3970: 103.49712653066325\n",
      "Cost after epoch 3980: 103.4305048249713\n",
      "Cost after epoch 3990: 103.46741282117128\n",
      "Cost after epoch 4000: 103.3985232317638\n",
      "Cost after epoch 4010: 103.4427811935772\n",
      "Cost after epoch 4020: 103.46894536681351\n",
      "Cost after epoch 4030: 103.50563758664745\n",
      "Cost after epoch 4040: 103.50209957981323\n",
      "Cost after epoch 4050: 103.37702931952805\n",
      "Cost after epoch 4060: 103.3544375585061\n",
      "Cost after epoch 4070: 103.39864960765783\n",
      "Cost after epoch 4080: 103.45271288193648\n",
      "Cost after epoch 4090: 103.46054713987331\n",
      "Cost after epoch 4100: 103.34643158306105\n",
      "Cost after epoch 4110: 103.43070135689159\n",
      "Cost after epoch 4120: 103.38094081177275\n",
      "Cost after epoch 4130: 103.45170979422375\n",
      "Cost after epoch 4140: 103.36707608576675\n",
      "Cost after epoch 4150: 103.47411539318287\n",
      "Cost after epoch 4160: 103.43480526267962\n",
      "Cost after epoch 4170: 103.42213012010689\n",
      "Cost after epoch 4180: 103.34016105749677\n",
      "Cost after epoch 4190: 103.42370606405069\n",
      "Cost after epoch 4200: 103.32200169730332\n",
      "Cost after epoch 4210: 103.3651265580705\n",
      "Cost after epoch 4220: 103.40303154450439\n",
      "Cost after epoch 4230: 103.36217526058931\n",
      "Cost after epoch 4240: 103.36354642722752\n",
      "Cost after epoch 4250: 103.42468335809778\n",
      "Cost after epoch 4260: 103.32949019604953\n",
      "Cost after epoch 4270: 103.4142098382341\n",
      "Cost after epoch 4280: 103.3845200052752\n",
      "Cost after epoch 4290: 103.37731206628727\n",
      "Cost after epoch 4300: 103.37603472831724\n",
      "Cost after epoch 4310: 103.39140461747073\n",
      "Cost after epoch 4320: 103.36151753709191\n",
      "Cost after epoch 4330: 103.37754462797666\n",
      "Cost after epoch 4340: 103.37355279913851\n",
      "Cost after epoch 4350: 103.40554146190078\n",
      "Cost after epoch 4360: 103.35907838043639\n",
      "Cost after epoch 4370: 103.36432030137563\n",
      "Cost after epoch 4380: 103.28567224593544\n",
      "Cost after epoch 4390: 103.26497688189515\n",
      "Cost after epoch 4400: 103.30596362971646\n",
      "Cost after epoch 4410: 103.37128483479218\n",
      "Cost after epoch 4420: 103.36475695457224\n",
      "Cost after epoch 4430: 103.42897275632359\n",
      "Cost after epoch 4440: 103.40646950780018\n",
      "Cost after epoch 4450: 103.38934003856684\n",
      "Cost after epoch 4460: 103.41685044788377\n",
      "Cost after epoch 4470: 103.44778423392073\n",
      "Cost after epoch 4480: 103.46326917589292\n",
      "Cost after epoch 4490: 103.44893878432316\n",
      "Cost after epoch 4500: 103.36325741715552\n",
      "Cost after epoch 4510: 103.45132298815417\n",
      "Cost after epoch 4520: 103.44658576850898\n",
      "Cost after epoch 4530: 103.39613813882131\n",
      "Cost after epoch 4540: 103.42891296766025\n",
      "Cost after epoch 4550: 103.43539253288905\n",
      "Cost after epoch 4560: 103.46550601153497\n",
      "Cost after epoch 4570: 103.44481598890619\n",
      "Cost after epoch 4580: 103.47391826098317\n",
      "Cost after epoch 4590: 103.3919146428881\n",
      "Cost after epoch 4600: 103.53878622504685\n",
      "Cost after epoch 4610: 103.41202573647094\n",
      "Cost after epoch 4620: 103.50628432221289\n",
      "Cost after epoch 4630: 103.4528776931269\n",
      "Cost after epoch 4640: 103.42035971246865\n",
      "Cost after epoch 4650: 103.47955182504016\n",
      "Cost after epoch 4660: 103.42279554882778\n",
      "Cost after epoch 4670: 103.4399600052207\n",
      "Cost after epoch 4680: 103.43138940705083\n",
      "Cost after epoch 4690: 103.53341654047043\n",
      "Cost after epoch 4700: 103.45947917703113\n",
      "Cost after epoch 4710: 103.47585407097252\n",
      "Cost after epoch 4720: 103.48203701715853\n",
      "Cost after epoch 4730: 103.49925170378405\n",
      "Cost after epoch 4740: 103.42698897468645\n",
      "Cost after epoch 4750: 103.4198092850115\n",
      "Cost after epoch 4760: 103.43883945706591\n",
      "Cost after epoch 4770: 103.35190989756234\n",
      "Cost after epoch 4780: 103.47726418821074\n",
      "Cost after epoch 4790: 103.40948253566575\n",
      "Cost after epoch 4800: 103.3791339633481\n",
      "Cost after epoch 4810: 103.35129938266138\n",
      "Cost after epoch 4820: 103.37593774145677\n",
      "Cost after epoch 4830: 103.37343025341104\n",
      "Cost after epoch 4840: 103.34026660697863\n",
      "Cost after epoch 4850: 103.34675603285218\n",
      "Cost after epoch 4860: 103.31782857893647\n",
      "Cost after epoch 4870: 103.26329269804565\n",
      "Cost after epoch 4880: 103.34130349462151\n",
      "Cost after epoch 4890: 103.30979976471382\n",
      "Cost after epoch 4900: 103.30519567057497\n",
      "Cost after epoch 4910: 103.2946630060058\n",
      "Cost after epoch 4920: 103.3223038648761\n",
      "Cost after epoch 4930: 103.28874153931142\n",
      "Cost after epoch 4940: 103.28577548409031\n",
      "Cost after epoch 4950: 103.27120609373151\n",
      "Cost after epoch 4960: 103.25807470780437\n",
      "Cost after epoch 4970: 103.26022505789207\n",
      "Cost after epoch 4980: 103.22223381316591\n",
      "Cost after epoch 4990: 103.23077914018506\n",
      "training time: 0:02:14.180461\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "paras = skipgram_model_training(X, Y_one_hot, vocab_size, 50, 0.05, 5000, batch_size=128, parameters=None, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X_test = np.arange(vocab_size)\n",
    "# X_test = np.expand_dims(X_test, axis=0)\n",
    "# print(X_test)\n",
    "# softmax_test, _ = forward_propagation(X_test, paras)\n",
    "# print(softmax_test.shape)\n",
    "# top_sorted_inds = np.argsort(softmax_test, axis=0)[-4:,:]\n",
    "# print(top_sorted_inds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.arange(vocab_size)\n",
    "X_test = np.expand_dims(X_test, axis=0)\n",
    "softmax_test, _ = forward_propagation(X_test, paras)\n",
    "top_sorted_inds = np.argsort(softmax_test, axis=0)[-4:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "theme's neighbor words: ['aliens', 'whose', 'competitions', 'a']\n",
      "1\n",
      "a's neighbor words: ['the', 'in', 'of', 'competition']\n",
      "2\n",
      "names's neighbor words: ['official', 'that', 'cities', 'of']\n",
      "3\n",
      "hosted's neighbor words: ['that', 'one', 'number', 'have']\n",
      "4\n",
      "descending's neighbor words: ['population', 'list', 'by', 'heads']\n",
      "5\n",
      "greatest's neighbor words: ['competitions', 'has', 'farm', 'in']\n",
      "6\n",
      "across's neighbor words: ['list', 'how', 'all', 'cows']\n",
      "7\n",
      "competition's neighbor words: ['the', 'what', 'one', 'official']\n",
      "8\n",
      "born's neighbor words: ['a', 'state', 'has', 'were']\n",
      "9\n",
      "year's neighbor words: ['is', 'competitions', 'order', 'by']\n",
      "10\n",
      "competitions's neighbor words: ['the', 'of', 'themes', 'year']\n",
      "11\n",
      "city's neighbor words: ['hosted', 'status', 'the', 'that']\n",
      "12\n",
      "head's's neighbor words: ['the', 'of', 'are', 'what']\n",
      "13\n",
      "cities's neighbor words: ['have', 'official', 'status', 'the']\n",
      "14\n",
      "budget's neighbor words: ['departments', 'many', 'who', 'for']\n",
      "15\n",
      "larger's neighbor words: ['are', 'competition', 'what', 'many']\n",
      "16\n",
      "the's neighbor words: ['of', 'are', 'what', 'and']\n",
      "17\n",
      "by's neighbor words: ['the', 'of', 'departments', 'are']\n",
      "18\n",
      "'yes''s neighbor words: ['acting', 'value', 'were', 'larger']\n",
      "19\n",
      "record's neighbor words: ['farm', 'theme', 'for', 'total']\n",
      "20\n",
      "states's neighbor words: ['the', 'where', 'least', 'number']\n",
      "21\n",
      "established's neighbor words: ['name', 'by', 'who', 'were']\n",
      "22\n",
      "managed's neighbor words: ['whose', 'secretary', 'for', 'a']\n",
      "23\n",
      "between's neighbor words: ['are', 'competition', 'what', 'many']\n",
      "24\n",
      "temporary's neighbor words: ['is', 'whose', 'value', 'in']\n",
      "25\n",
      "there's neighbor words: ['departments', 'number', 'and', 'count']\n",
      "26\n",
      "do's neighbor words: ['count', 'competitions', 'different', 'cities']\n",
      "27\n",
      "please's neighbor words: ['different', 'show', 'themes', 'city']\n",
      "28\n",
      "has's neighbor words: ['head', 'most', \"'ha'\", 'which']\n",
      "29\n",
      "secretary's neighbor words: ['department', \"security'\", 'state', 'both']\n",
      "30\n",
      "populations's neighbor words: ['what', 'competitions', 'cities', 'horses']\n",
      "31\n",
      "hosts's neighbor words: ['for', 'aliens', 'whose', 'return']\n",
      "32\n",
      "ages's neighbor words: ['heads', 'the', 'are', 'born']\n",
      "33\n",
      "frequency's neighbor words: ['most', 'name', 'farm', 'common']\n",
      "34\n",
      "count's neighbor words: ['of', 'the', 'list', 'are']\n",
      "35\n",
      "'homeland's neighbor words: ['born', \"security'\", 'secretary', 'the']\n",
      "36\n",
      "least's neighbor words: ['born', 'states', 'across', 'were']\n",
      "37\n",
      "substring's neighbor words: ['the', 'of', 'are', 'what']\n",
      "38\n",
      "were's neighbor words: ['which', 'department', 'in', 'year']\n",
      "39\n",
      "how's neighbor words: ['are', 'statuses', 'many', 'the']\n",
      "40\n",
      "different's neighbor words: ['cities', 'the', 'statuses', 'of']\n",
      "41\n",
      "security''s neighbor words: ['which', 'secretary', \"'homeland\", 'were']\n",
      "42\n",
      "many's neighbor words: ['how', 'there', 'farms', 'led']\n",
      "43\n",
      "in's neighbor words: ['what', 'order', 'were', 'return']\n",
      "44\n",
      "age's neighbor words: ['heads', 'state', 'creation', 'born']\n",
      "45\n",
      "all's neighbor words: ['the', 'many', 'across', 'return']\n",
      "46\n",
      "name's neighbor words: ['of', 'the', 'and', 'number']\n",
      "47\n",
      "themes's neighbor words: ['competitions', 'with', 'that', 'have']\n",
      "48\n",
      "time's neighbor words: ['id', 'substring', 'names', 'head']\n",
      "49\n",
      "years's neighbor words: ['departments', 'official', 'competitions', 'names']\n",
      "50\n",
      "employees's neighbor words: ['departments', 'many', 'who', 'for']\n",
      "51\n",
      "are's neighbor words: ['the', 'of', 'what', 'and']\n",
      "52\n",
      "what's neighbor words: ['the', 'are', 'official', 'is']\n",
      "53\n",
      "is's neighbor words: ['what', 'the', 'horses', 'of']\n",
      "54\n",
      "type's neighbor words: ['across', 'common', 'hosted', 'most']\n",
      "55\n",
      "any's neighbor words: ['are', 'competition', 'what', 'many']\n",
      "56\n",
      "outside's neighbor words: ['state', 'california', 'across', 'who']\n",
      "57\n",
      "status's neighbor words: ['the', 'city', 'cities', 'of']\n",
      "58\n",
      "farm's neighbor words: ['what', 'by', 'competition', 'the']\n",
      "59\n",
      "number's neighbor words: ['of', 'across', 'the', 'average']\n",
      "60\n",
      "total's neighbor words: ['horses', 'what', 'are', 'for']\n",
      "61\n",
      "order's neighbor words: ['the', 'what', 'themes', 'of']\n",
      "62\n",
      "where's neighbor words: ['heads', 'secretary', 'states', 'list']\n",
      "63\n",
      "corresponding's neighbor words: ['with', 'host', 'order', 'largest']\n",
      "64\n",
      "that's neighbor words: ['the', 'hosted', 'of', 'have']\n",
      "65\n",
      "minimum's neighbor words: ['the', 'cows', 'maximum', 'and']\n",
      "66\n",
      "departments's neighbor words: ['by', 'heads', 'what', 'employees']\n",
      "67\n",
      "both's neighbor words: ['of', 'the', 'list', 'are']\n",
      "68\n",
      "one's neighbor words: ['are', 'the', 'what', 'of']\n",
      "69\n",
      "cows's neighbor words: ['farms', 'average', 'all', 'across']\n",
      "70\n",
      "on's neighbor words: ['horses', 'more', 'ascending', 'a']\n",
      "71\n",
      "head's neighbor words: ['time', 'at', 'any', \"'ha'\"]\n",
      "72\n",
      "department's neighbor words: ['secretary', 'than', 'the', 'of']\n",
      "73\n",
      "and's neighbor words: ['the', 'of', 'name', 'cities']\n",
      "74\n",
      "have's neighbor words: ['that', 'the', 'cities', 'than']\n",
      "75\n",
      "whose's neighbor words: ['not', 'temporary', 'rank', 'between']\n",
      "76\n",
      "which's neighbor words: ['were', 'has', 'born', 'is']\n",
      "77\n",
      "who's neighbor words: ['heads', 'outside', 'mentioned', 'list']\n",
      "78\n",
      "ascending's neighbor words: ['the', 'is', 'list', 'of']\n",
      "79\n",
      "host's neighbor words: ['more', 'competitions', 'populations', 'having']\n",
      "80\n",
      "ordered's neighbor words: ['population', 'statuses', 'number', 'list']\n",
      "81\n",
      "of's neighbor words: ['the', 'cities', 'and', 'number']\n",
      "82\n",
      "distinct's neighbor words: ['the', 'of', 'list', 'are']\n",
      "83\n",
      "average's neighbor words: ['working', 'number', 'employees', 'population']\n",
      "84\n",
      "maximum's neighbor words: ['number', 'minimum', 'and', 'budget']\n",
      "85\n",
      "not's neighbor words: ['are', 'the', 'what', 'farm']\n",
      "86\n",
      "farms's neighbor words: ['than', 'horses', 'the', 'of']\n",
      "87\n",
      "than's neighbor words: ['the', 'show', 'of', 'more']\n",
      "88\n",
      "mentioned's neighbor words: ['the', 'of', 'are', 'what']\n",
      "89\n",
      "sorted's neighbor words: ['ascending', 'farm', 'in', 'year']\n",
      "90\n",
      "having's neighbor words: ['than', 'larger', 'populations', 'born']\n",
      "91\n",
      "each's neighbor words: ['the', 'show', 'different', 'are']\n",
      "92\n",
      "older's neighbor words: ['the', 'of', 'are', 'what']\n",
      "93\n",
      "give's neighbor words: ['and', 'the', 'number', 'average']\n",
      "94\n",
      "rank's neighbor words: ['and', 'the', 'are', 'what']\n",
      "95\n",
      "with's neighbor words: ['total', 'residents', 'than', 'population']\n",
      "96\n",
      "official's neighbor words: ['the', 'cities', 'of', 'names']\n",
      "97\n",
      "common's neighbor words: ['all', 'status', 'most', 'type']\n",
      "98\n",
      "statuses's neighbor words: ['different', 'how', 'have', 'names']\n",
      "99\n",
      "value's neighbor words: ['how', 'temporary', 'whose', \"'yes'\"]\n",
      "100\n",
      "return's neighbor words: ['the', 'of', 'and', 'statuses']\n",
      "101\n",
      "state's neighbor words: ['the', 'are', 'of', 'what']\n",
      "102\n",
      "largest's neighbor words: ['is', 'whose', 'value', 'in']\n",
      "103\n",
      "held's neighbor words: ['what', 'host', 'competitions', 'cities']\n",
      "104\n",
      "for's neighbor words: ['managed', 'theme', 'sorted', 'employees']\n",
      "105\n",
      "aliens's neighbor words: ['the', 'hosts', 'theme', 'what']\n",
      "106\n",
      "led's neighbor words: ['who', 'whose', 'california', 'time']\n",
      "107\n",
      "california's neighbor words: ['are', 'competition', 'what', 'many']\n",
      "108\n",
      "most's neighbor words: ['show', 'the', 'common', 'of']\n",
      "109\n",
      "at's neighbor words: ['were', 'list', 'states', 'heads']\n",
      "110\n",
      "working's neighbor words: ['farms', 'all', 'average', 'across']\n",
      "111\n",
      "acting's neighbor words: [\"'yes'\", 'states', 'there', 'value']\n",
      "112\n",
      "heads's neighbor words: ['born', 'acting', 'who', 'departments']\n",
      "113\n",
      "population's neighbor words: ['the', 'with', 'of', 'what']\n",
      "114\n",
      "list's neighbor words: ['the', 'name', 'born', 'by']\n",
      "115\n",
      "'ha''s neighbor words: ['id', 'substring', 'names', 'head']\n",
      "116\n",
      "horses's neighbor words: ['the', 'each', 'number', 'total']\n",
      "117\n",
      "residents's neighbor words: ['the', 'years', 'with', 'are']\n",
      "118\n",
      "'treasury''s neighbor words: ['the', 'of', 'are', 'what']\n",
      "119\n",
      "'alabama''s neighbor words: ['the', 'of', 'are', 'what']\n",
      "120\n",
      "show's neighbor words: ['the', 'statuses', 'and', 'of']\n",
      "121\n",
      "creation's neighbor words: ['the', 'and', 'of', 'are']\n",
      "122\n",
      "more's neighbor words: ['than', 'horses', 'that', 'competition']\n",
      "123\n",
      "id's neighbor words: ['how', 'the', 'of', 'and']\n"
     ]
    }
   ],
   "source": [
    "for input_ind in range(vocab_size):\n",
    "    print(input_ind)\n",
    "    input_word = id_to_word[input_ind]\n",
    "    output_words = [id_to_word[output_ind] for output_ind in top_sorted_inds[::-1, input_ind]]\n",
    "    print(\"{}'s neighbor words: {}\".format(input_word, output_words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
